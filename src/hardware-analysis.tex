\chapter{The Cold-Start Problem in Hardware}\label{ch:hw-analysis}

\section{Initial profiling}\label{sec:initial-profiling}
Firstly, we need to quantify and detect whether the cold-start problem exists in GPGPU hardware.
To this end, we've analyzed some workloads and benchmarks using the NVIDIA Nsight Compute tool.
This tool allows us to control the caches during the execution of an application.
Additionally, we can gather detailed metrics, like IPC, for each kernel in the workload.

Each workload was profiled twice: once like it would run normally, and once with the caches flushed between kernel invocations.
Nsight Compute supports this option by using the \verb|--cache-control=all| argument.
To keep the runtime in check, we limited the execution to 20\thinspace000 kernels per workload.
These experiments were run on an NVIDIA GeForce RTX 3080\cite{nvidia-wp}.
This GPU has 68 SM cores, 6 MB of L2 cache.

After profiling, we attempted to match each profiled non-flushed kernel with its flushed counterpart.
This proved easier said than done, as the kernel IDs were not consistent between the two profiles.

\begin{figure}
    \centering
    \resizebox{0.45\textwidth}{!}{\input{./figures/hardware/mlperf_cactus_boxplot.pgf}}
    \resizebox{0.45\textwidth}{!}{\input{./figures/hardware/dct_ocean_boxplot.pgf}}
    \caption{Relative IPC difference}
    \label{fig:ipc_diff}
\end{figure}

From this initial analysis, the most interesting workloads seemed to be PyTorch DCGAN\cite{dcgan} and Gunrock\cite{gru} (on road traversal), from the Cactus\cite{cactus} suite.
Below is a full list of all analyzed workloads:
\begin{itemize}
    \item Gromacs\cite{gromacs} and LAMMPS\cite{LAMMPS} (with both rhodo (LMR) and colloid (LMC) inputs); two molecular simulation workloads,
    \item Gunrock on both road (GRU) and social networks (GST),
    \item DCGAN, neural style transform (NST)\cite{nst}, reinforcement learning (RFL), spatial transformer (SPT)\cite{spt} and language translation (LGT) from PyTorch, and
    \item The following MLCommons benchmarks (from their MLPerfÂ® Inference v2.0 collection):
    \begin{itemize}
        \item The ResNet50 model\cite{resnet50},
        \item Both MobileNet and ResNet34 variants of the SSD model,
        \item The Bidirectional Encoder Representations from Transformers (BERT)\cite{bert}, and
        \item The 3D U-Net model\cite{3d-unet}
    \end{itemize}
    \item Four inputs to the 8x8 DCT implementation in the CUDA Samples (each labeled with their respective input).
\end{itemize}

In \cref{fig:ipc_diff}, you can see the relative IPC difference for each kernel in the Cactus and MLPerf workloads.
For the majority of workloads, the IPC difference is rather small (at most 10\%).
However, for some of the more interesting workloads (like DCG, GRU, NST, and DCT), we can find IPC differences of up to 70\% for some kernels.
\FloatBarrier

\section{Weighting kernels}\label{sec:weighting-kernels}
While this shows that the problem does exist in hardware, it might not relate to how modern simulation is carried out.
Modern techniques like Sieve\cite{sieve} use clustering to select a subset of kernels to simulate, generalizing the results to the entire workload.
In these sampling techniques, many kernels are clustered together based on characteristics.
The process then selects a single kernel from each cluster to simulate, using those results are representative for the entire cluster.
To combine the clusters and compute a final result, each cluster's result is weighted by its instruction count, relative to the full workload:
\begin{align*}
    w_{kernel} &= \frac{\text{kernel instruction count}}{\sum_{\text{kernel } k \in \text{ workload}}{\text{kernel}_k \text{ instruction count}}} \\
    w_{cluster} &= \sum_{\text{kernel } k \in \text{ cluster}} w_{kernel}
\end{align*}
In order to get a better view of the impact of the cold start problem, we've also computed IPC difference when each kernel is weighted by its instruction count.

\begin{figure}[t]
    \centering
    \begin{subfigure}{\textwidth}
        \begin{minipage}[c]{0.45\textwidth}
            \resizebox{\textwidth}{!}{\input{figures/hardware/table_mlperf_cactus}}
        \end{minipage}
        \begin{minipage}[c]{0.45\textwidth}
            \resizebox{\textwidth}{!}{\input{figures/hardware/mlperf_cactus_weight.pgf}}
        \end{minipage}
        \caption{Weighted IPC differences}
        \label{fig:weight_ipc_diff}
    \end{subfigure}
    \centering
    \begin{subfigure}{\textwidth}
        \begin{minipage}[c]{0.45\textwidth}
            \resizebox{\textwidth}{!}{\input{figures/hardware/table_dct_ocean}}
        \end{minipage}
        \begin{minipage}[c]{0.45\textwidth}
            \resizebox{\textwidth}{!}{\input{figures/hardware/dct_ocean.pgf}}
        \end{minipage}
        \caption{Weighted IPC differences for different DCT inputs}
        \label{fig:weight_ipc_dct}
    \end{subfigure}
    \caption{Weighted IPC differences}
\end{figure}

In \cref{fig:weight_ipc_diff}, you can see the result of this analysis for the Cactus and MLPerf workloads.
The results for the DCT workload (with its same four input images) is shown in \cref{fig:weight_ipc_dct}.
We've set four thresholds for the relative IPC difference (5\%, 10\%, 15\%, and 20\%).
For each of these thresholds, we've summed up the weights of all kernels where the IPC difference is at least that much.
This means that e.g.\ for the GRU workload, approximately 16\% of the entire workload suffers from a difference of at least 5\%.

From these figures, we end up with a different set of affected workloads.
Most of the MLPerf workloads only suffer slightly from the cold-start problem, with the only notable exception being the 3D U-Net workload.
In the Cactus suite, we found language translation (LGT), spatial transformer (SPT), reinforcement learning (RFL), and Gunrock (with road input, GRU) to be the most affected.
The real outlier here, however, is the DCT one.
It consistently suffers from at least 20\% relative IPC difference, no matter the input.

\FloatBarrier

\section{Data reuse}\label{sec:data-reuse}
We assume that the cold-start problem is more prevalent in workloads with high data reuse.
To verify this, we've analyzed the degree of inter-kernel data reuse in the DCT workload.
For each kernel, we've profiled all memory instructions and extracted the unique memory addresses.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{\input{./figures/hardware/dct_forward_reuse.pgf}}
        \caption{Forward data reuse}
        \label{fig:dct_forward_reuse}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{\input{./figures/hardware/dct_backward_reuse.pgf}}
        \caption{Backward data reuse}
        \label{fig:dct_backward_reuse}
    \end{subfigure}
    \caption{Inter-kernel data reuse in DCT}
    \label{fig:dct_reuse}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{\input{./figures/hardware/recg_forward_reuse.pgf}}
        \caption{Forward data reuse}
        \label{fig:recg_forward_reuse}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{!}{\input{./figures/hardware/recg_backward_reuse.pgf}}
        \caption{Backward data reuse}
        \label{fig:recg_backward_reuse}
    \end{subfigure}
    \caption{Inter-kernel data reuse in recursiveGaussian}
    \label{fig:recg_reuse}
\end{figure}

In \cref{fig:dct_reuse}, you can see the data reuse ratio for the DCT workload.
We've analyzed both the forward and backward data reuse for each kernel:
\begin{itemize}
    \item \textbf{Forward data reuse:} the amount of unique memory addresses in kernel $k_{i-1}$ that are also present in kernel $k_{i}$ (as seen in \cref{fig:dct_forward_reuse}); i.e.
    \begin{align}
        \frac{|\text{footprint } k_{i-1} \cap \text{footprint } k_i|}{|\text{footprint } k_{i-1}|}
    \end{align}
    \item \textbf{Backward data reuse:} the amount of unique memory addresses in kernel $k_{i}$ that are also present in kernel $k_{i-1}$ (as seen in \cref{fig:dct_backward_reuse}); i.e.
    \begin{align}
        \frac{|\text{footprint } k_{i-1} \cap \text{footprint } k_i|}{|\text{footprint } k_i|}
    \end{align}
\end{itemize}

When looking at the DCT workload, most kernels have a very high degree of data reuse, both backward and forward.
As we analyzed before, this workload suffers heavily from the cold-start problem (in hardware).

For contrast, we've also analyzed \verb|recursiveGaussian| (from the CUDA SDK); as you can see in \cref{fig:recg_reuse}.
Roughly half of the kernels hit 50\% of data reuse (in either direction), while the other half has a much lower degree of data reuse.
This is also reflected in the IPC difference due to the cold-start problem: only about 1.3\% of the workload suffers from at least 5\% IPC difference.

The notion of data reuse, more specifically forward data reuse, will be used in the final chapter, when we discuss possible mitigations to the cold-start problem.

\FloatBarrier
\section{Hardware conclusion}\label{sec:hw-conclusion}
From this, we can conclude that the cold-start problem does exist in hardware.
Additionally, we noticed that the problem changes severity when we take each relative kernel's instruction count into account, giving us other workloads to focus on.
Finally, we suspected that workloads with high inter-kernel data reuse would suffer more from the cold-start problem.
This was confirmed by the DCT workload, as we've shown in \cref{fig:dct_reuse}.

In the next chapter, we'll be analyzing the impact of the cold-start problem in the AccelSim simulator.
We'll mostly use the DCT and 3D U-Net workloads for this, as well as the OceanFFT one.
