\chapter{Mitigation}\label{ch:mitigation}

So far, we have talked about the cold-start problem in both hardware and simulation.
The reason why we have looked into it, is because of how we approach simulating large workloads in GPUs.
Instead of simulating each and every kernel, we only simulate a select subset.
Usually, these are selected using both profiling and some form of clustering based on characteristics.
However, since we only simulate intermittent kernels, cache state might be lost and/or incorrect; which leads to inaccuracies.

Taking this all into account means that we see a few options for mitigation:
\begin{itemize}
    \item \textbf{Simulate preceding kernels:} this is the most naive, but also the most computationally intensive approach.
    Simulating all instructions from one or more kernels that come right before the one we need ensures that the cache state is as close to the real state as possible, but also increases the simulation time.
    \item \textbf{Simulate certain instructions from preceding kernels:} this option is more refined than the previous, finding a balance between computation time and accuracy.
    Since we have the full trace of each kernel, we can select only the memory instructions from the preceding kernels, simulating those to warm up the caches artificially.
    Especially when kernels contain a lot of computations, and fewer memory loads/stores, this could allow for a higher accuracy at a very low cost.
    \item \textbf{Compute a correction factor:} logic dictates that there should be a way to compute a correction factor for the cold-start problem.
    There are different factors that we could analyze ahead-of-time to compute this factor.
\end{itemize}

In \Cref{sec:trace-info,sec:kernel-selection}, we will focus on the second option, opting to simulate memory instructions in order to artificially warm up the caches.
\Cref{sec:correction-factor} will focus on the third option, offering some possible factors that could be used to compute a correction factor.

\FloatBarrier
\section{Gathering trace info}\label{sec:trace-info}
The AccelSim~\cite{accelsim} tool comes with an NVBit~\cite{nvbit} tool and post-processor which are already used to generate the instruction-level traces that are eventually fed to the simulator itself.
By default, this tool generates a number of files:
\begin{itemize}
    \item \verb|traces/kernelslist|: a list of all kernels that were run, with their respective kernel IDs.
    \item \verb|traces/stats.csv|: a CSV file containing some global statistics about each kernel.
    \item \verb|traces/kernel-<number>.trace|: the actual instruction-level trace for each kernel, identified by their numbers.
\end{itemize}

We have expanded this tool to output an additional file for each kernel: \verb|traces/kernel-mem-<number>.trace|.
This file contains all memory instructions issued by the selected kernel, as well as the final \verb|EXIT| instruction (omitting this one would lead to a segmentation fault in the simulator).
The original NVBIT tool reports for each instruction it instruments whether it is a memory instruction or not.
We use that information to filter out the instructions we need.

As with the original traces, we need to post-process each memory trace before feeding it to the simulator.
This post-processing step sorts the instructions by their thread block, allowing the simulator to easily access the next instruction.
By using the same format as the original tool, we ensure that the existing post-processor can also handle these new files\footnote{
    \textit{Note:} AccelSim by default uses a compressed format for memory instructions, taking up less disk space in their traces.
    However, due to time constraints, we have elected to work with the uncompressed format, which takes up more disk space.
}.

\FloatBarrier
\section{Kernel selection}\label{sec:kernel-selection}
In order to warm up the simulator's caches, we will be simulating the memory instructions from preceding kernels.
However, this is once again a trade-off: the more kernels we use to warm up the caches, the more accurate the simulation will be, but the longer it will take.
We have selected four kernels from the DCT8x8 workload with high IPC differences, as shown in \Cref{fig:dct_kernels}.

\begin{figure}
    \centering
    \append{figures/simulated/dct_kernels}
    \caption{High IPC difference kernels in DCT}
    \label{fig:dct_kernels}
\end{figure}

For each of these kernels, we used multiple simulations:
\begin{itemize}
    \item \textbf{Perfect warmup:} the kernel is simulated in order, with all preceding kernels simulated in full.
    \item \textbf{Full memory warmup:} the kernel is simulated in order, with all memory instructions preceding it simulated (for kernel $i$, this means all memory instructions from kernels $1$ until $i - 1$ are simulated).
    \item \textbf{Partial memory warmup:} the memory instructions of up to 10 preceding kernels are simulated.
\end{itemize}

\begin{figure}[t]
    \centering
    \appendW{./figures/simulated/kernel-11-abs.pgf}{0.48}
    \appendW{./figures/simulated/kernel-13-abs.pgf}{0.48}
    \appendW{./figures/simulated/kernel-114-abs.pgf}{0.48}
    \appendW{./figures/simulated/kernel-115-abs.pgf}{0.48}
    \append{./figures/simulated/mitigation_absolute}
    \caption{DCT Mitigation results (absolute IPC values)}
    \label{fig:mitig_abs}
\end{figure}

\begin{figure}[t]
    \centering
    \appendW{./figures/simulated/kernel-11-acc.pgf}{0.48}
    \appendW{./figures/simulated/kernel-13-acc.pgf}{0.48}
    \appendW{./figures/simulated/kernel-114-acc.pgf}{0.48}
    \appendW{./figures/simulated/kernel-115-acc.pgf}{0.48}
    \append{./figures/simulated/mitigation_accuracy}
    \caption{DCT Mitigation results (accuracy)}
    \label{fig:mitig_acc}
\end{figure}

In \Cref{fig:mitig_abs,fig:mitig_acc}, we have plotted the results of these simulations.
For each kernel, we have plotted the result of \textit{perfect warmup} as a black line, this was the golden reference we tried to reach.
Additionally, the light-brown line represents \textit{full memory warmup}, while the blue line represents \textit{partial memory warmup}.
Both raw IPC values and accuracy (in percents) are shown.

From these figures, we can quickly deduce that even a single preceding kernel can lead to drastic accuracy increases: most kernels reach over 99\% accuracy with just one kernel warmed up.

An additional striking detail is that more kernels does not always imply a higher accuracy.
This might be due to some non-determinism in the simulator.

\FloatBarrier
\subsection{Full Memory Warmup}\label{subsec:full-memory-warmup}
A striking result from the figures above, is that the \textit{full memory warmup} approach does not yield 100\% accuracy.
The cause of this might be the ordering of instructions; since we remove all non-memory instructions, they might get executed in a different order.
This could lead to different cache states, and thus different IPC values.

In the example in \Cref{fig:example2a}, we have two threads executing.
Each of these has a different pattern: one starts with some memory instructions, performing arithmetic afterward, while the other does the opposite.
We assume that each instruction takes the exact same time, ignoring latencies as well.
This causes all memory instructions to be executed serially, resulting in a certain cache state.
\begin{figure}[hb]
    \centering
    \begin{minipage}[!]{0.45\textwidth}
        \begin{minted}[linenos]{nasm}
            ; Thread 1
            ld 0x0001
            ld 0x0002
            ld 0x0003
            ; arithmetic
            ; arithmetic
            ; arithmetic
        \end{minted}
    \end{minipage}
    \begin{minipage}[!]{0.45\textwidth}
        \begin{minted}[linenos]{nasm}
            ; Thread 2
            ; arithmetic
            ; arithmetic
            ; arithmetic
            ld 0x000A
            ld 0x0014
            ld 0x0028
        \end{minted}
    \end{minipage}
    \appendW{figures/simulated/interleaving-1}{0.66}
    \caption{Interleaving example}
    \label{fig:example2a}
\end{figure}

When we execute only the memory instructions in the ``trace'' above, we get a whole different execution.
See \Cref{fig:example2b} for the result of that execution.
\begin{figure}[ht]
    \centering
    \begin{minipage}[!]{0.45\textwidth}
        \begin{minted}[linenos]{nasm}
            ; Thread 1
            ld 0x0001
            ld 0x0002
            ld 0x0003
        \end{minted}
    \end{minipage}
    \begin{minipage}[!]{0.45\textwidth}
        \begin{minted}[linenos]{nasm}
            ; Thread 2
            ld 0x000A
            ld 0x0014
            ld 0x0028
        \end{minted}
    \end{minipage}
    \appendW{figures/simulated/interleaving-2}{0.45}
    \caption{Interleaving example - memory only}
    \label{fig:example2b}
\end{figure}

When we compare the final cache state of each execution (see also \Cref{fig:example2_final}), we can see a clear difference.
This can impact the performance of the next kernel: for the first execution (a perfect warmup), accessing \verb|0x000A| would result in a cache hit, while \verb|0x0002| would miss.
The opposite happens when we only simulate memory instructions (full memory warmup, like in execution 2).
This can explain the slight inaccuracy we see for full memory warmup.
\begin{figure}[h]
    \centering
    \appendW{figures/simulated/interleaving-delta}{0.36}
    \caption{Interleaving example - cache state differences}
    \label{fig:example2_final}
\end{figure}

\FloatBarrier
\section{Correction factor}\label{sec:correction-factor}

Since the cold-start problem is inherently tied to the cache state, we should look at factors that can impact this state in order to find a correction factor.
Tools like NVIDIA's Nsight Compute~\cite{nsight} and NVBit~\cite{nvbit} can provide detailed cache statistics, which might be used to compute this factor.
Some possible factors we considered are:
\begin{itemize}
    \item \textit{Profiler differences:} by profiling a workload twice, once with and once without flushing, one can see the impact of the cold-start problem in silicon.
    This data might be used to improve simulation results.
    \item \textit{Profiler cache statistics:} a detailed profiler can provide statistics about hits and misses, which in turn could be used to compute the correction factor.
    \item \textit{Degree of data reuse:} by using an instruction-level trace, it is possible to extract all memory references.
    The ratio $\frac{\text{total memory references}}{\text{unique memory references}}$ could inform us about the cache state.
    We have outlined the main ideas of \textit{forward data reuse} in \Cref{sec:data-reuse} (that section also includes the notion of \textit{backward data reuse}, but that will prove not as useful here).
    \item \textit{The DRAM delay:} the difference in cycles between a cache hit and a DRAM access could be useful to get an idea of the amount of cycles lost due to cold-start.
    \item \textit{The number of memory controllers:} since each memory controller can issue requests in parallel, the number of controllers might impact the cache state, as well as the cost of a cache miss.
    \item \textit{L2 miss rate:} this value could be used together with the \textit{number of memory instructions}.
\end{itemize}

Some of these factors will be used in \Cref{subsec:formula}, where we outline a formula that seems to perform rather well.
All the data used for that formula can be gathered by simulating a single kernel, and/or profiling the workload.

However, we have to be careful when finding a correction factor.
Since some factors that could impact cache state (e.g.\ the number of cache evictions) are hardware- and platform-dependent, we could end up with a correction factor that only works for a specific platform.

\subsection{A Possible Formula}\label{subsec:formula}
Through analysis of the data, we have come up with a formula that allows use to compute a correction factor.
This correction factor proved very effective, resulting in an average 95\% accuracy for the DCT workload.
Especially for problematic kernels (kernels suffering from the cold-start problem), this correction factor succeeded in improving accuracy.

The main goal is to compute a subtractive factor for the number of cycles, i.e.\ provide an upper bound on how many cycles could have been saved.
To this end, we will use the following metrics, all of which can be gathered by simulating a single kernel and/or profiling the workload:
\begin{itemize}
    \item \textbf{Unique DRAM accesses:} we modified Accel-Sim to output each DRAM request.
    By filtering these on unique values, we know the number of cold misses.
    \item \textbf{Memory controller occupation:} since the memory controllers in the system are able to process requests in parallel, the number of controllers might impact the number of cycles lost due to latency.
    Accel-Sim outputs their occupancy by default (the lines starting with\ \verb|dram[]|).
    Importantly, we also take into account how evenly they are used; see \Cref{subsubsec:mem-ctrl}.
    \item \textbf{Cache line size:} the DRAM accesses contain byte-level memory addresses.
    However, multiple consecutive addresses map to the same cache line, which might avoid some cold misses.
    \item \textbf{Reuse factor:} we use the \textit{forward reuse factor} from the \emph{previous} kernel.
    This value acts as a correction on the number of cold misses: if only a few memory addresses are reused, most cold misses aren't due to the cold-start problem.
    \item \textbf{Latencies:} the number of cycles saved is directly related to the DRAM- and L2-latency of the system.
\end{itemize}

Combining all of these, we came up with the following formula:
\begin{align}
    IPC &= \frac{instructions}{cycles_f - \Delta} \\
    \Delta &= \frac{accesses}{controllers * line} \cdot fwd_{i-1} \cdot (DRAM - L2)
\end{align}
Here, $IPC$ is the final (corrected) IPC value, $instructions$ is the number of dynamically executed instructions, $cycles_f$ is the number of cycles when simulating the kernel with cold caches (in flushed mode), and $\Delta$ is the correction factor.
We use $accesses$ for the number of unique DRAM accesses, $controllers$ for the memory controller occupancy, $line$ for the size of a cache line, and $reuse$ for the (forward) reuse factor.
$DRAM$ and $L2$ are the latencies of the respective memory regions.

This formula was tested on the DCT workload and improved on the sensitive kernels (11, 13, 114, and 115).
The other kernels suffered from a slight drop in accuracy, when compared to their raw accuracy (i.e.\ using no correction factor at all).

\textit{Note:} the formula above cannot be used for OceanFFT, as it uses a subtractive factor.
In OceanFFT, we would need to increase the number of cycles (since starting from a cold cache increases the IPC for some kernels).

In \Cref{fig:dct-mitig}, we have gathered some results for the DCT workload.
For a number of kernels, the accuracy of the formula is compared to the raw accuracy (meaning the flushed accuracy, without any corrections).
As you can see, the overall accuracy took a slight hit (dropping from 98\% to around 96\%), while the sensitive kernels improved (from 89\% to 93\%).
In \Cref{sec:sim-vs-corr}, we will compare the accuracy of this approach against the accuracy for memory warmup (with a single kernel).

\begin{figure}[h]
    \centering
    \appendW{figures/mitigation/corr_dct}{}
    \caption{DCT mitigation results}
    \label{fig:dct-mitig}
\end{figure}

The exact values are slightly different from the ones used in \Cref{sec:kernel-selection}, but the general trend is the same.
This is due to the inherent non-determinism in the simulator, which can lead to slightly different results for the same kernel.

\subsubsection{Memory controllers}\label{subsubsec:mem-ctrl}
Not all workloads use all memory controllers evenly - in the worst case, a single controller does all the work while the others are idle.
In that case, it would be incorrect to use the total number of controllers in the formula.
To mitigate this, we compute the evenly used controllers, and use that value in the formula.

AccelSim outputs memory controller data under the form of number of DRAM requests made, per controller, per bank.
Additionally, this data is output for both reads and writes.

We compute the evenly used controllers from the average occupancy of each controller ($occ_{ctrl}$) as follows:
\begin{align}
    occ_{ctrl} &= \frac{\sum_{i = 1}^{banks}{reads_{ctrl, i}} + \sum_{i = 1}^{banks}{writes_{ctrl, i}}}{2 \cdot banks} \\
    controllers &= \frac{\sum_{ctrl}{occ_{ctrl}}}{\max_{ctrl}{occ_{ctrl}}}
\end{align}

Where:
\begin{itemize}
    \item $occ_{ctrl}$ is the average occupancy of a controller $ctrl$;
    \item $banks$ is the number of memory banks;
    \item $reads_{ctrl, i}$ is the number of read accesses made by controller $ctrl$ to bank $i$;
    \item $writes_{ctrl, i}$ is the number of write accesses made by controller $ctrl$ to bank $i$; and
    \item $controllers$ is the number of evenly used controllers (the final value used in the formula for $\Delta$).
\end{itemize}

For DCT, this value was around 8 in our experimental setup, meaning that 8 out of the 16 controllers were used.
OceanFFT, however, used almost all controllers evenly.

\FloatBarrier
\section{Comparison}\label{sec:sim-vs-corr}
\begin{figure}[h]
    \centering
    \appendW{figures/mitigation/comp-box.pgf}{}
    \appendW{figures/mitigation/comp-table}{0.9}
    \caption{Comparison of mitigation methods}
    \label{fig:mitig-compare}
\end{figure}

In \Cref{fig:mitig-compare}, we see that both mitigation approaches are able to eliminate the (negative) outliers.
However, since there are so few suffering kernels, the median doesn't improve by a lot.
In general, we can conclude that either mitigation approach is able to improve the accuracy of the simulator.

Additionally, we also included minimum accuracy, the first quartile, the median, the standard deviation, the third quartile, and the maximum accuracy for each of these methods.
Due to the elimination of outliers, the standard deviation is lower for both mitigation methods, compared to the raw accuracy.

Due to temporal locality, we expect the impact of a kernel on the caches to be lower the further back in time it is.
This is also reflected in the results: \Cref{fig:mitig-compare} clearly shows that full memory warmup has no practical benefits over just using a single kernel to warm up the caches.
Worse still, all metrics are slightly worse for full memory warmup, compared to partial memory warmup.

\FloatBarrier
\section{Conclusion}\label{sec:mitigation-conclusion}

We have looked into three possible mitigations, and analyzed their strengths and weaknesses.
First, there is the naive approach of simulating all preceding kernels in full, guaranteeing a correct warmup, and thus a ``perfectly accurate'' simulation.
However, this has the drawback of being incredibly computationally intensive, negating the work done by sampling kernels in the first place.

Secondly, we have looked into simulating only parts of the previous kernels.
In this case, we have selected the memory instructions of preceding kernels and simulated those to artificially warm up the caches.
Even for kernels that suffered a lot from the cold-start problem, we have shown that warming up using a single kernel has a drastic impact on the accuracy already.
In many cases, a single kernel was enough to reach over 99\% accuracy.

Lastly, we have looked into computing a correction factor.
The factor we proposed is based on the number of cycles that could be saved, taking into account parallel requests and DRAM information.
This factor has less of a computational overhead (since no additional traces need to be simulated), but resulted in a slightly lower accuracy than the memory warmup approach.
However, we need to be careful not to over-tune this factor, as it could limit the simulator to a specific platform (beating the purpose of GPU simulation in the first place).

In general, both approaches are able to mitigate the cold-start problem, and improve the accuracy of the simulator, especially for kernels that do suffer from it.
Either method was able to raise the accuracy of each DCT kernel to at least 90\%, with the memory warmup reaching up to almost 95\%.